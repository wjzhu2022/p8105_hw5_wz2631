---
title: "homework5"
author: "wz2631"
date: "2022-11-16"
output: github_document
---

```{r, include=FALSE}
library(tidyverse)
library(viridis)
library(purrr)
library(tidyr)
library(broom)

knitr::opts_chunk$set(
	fig.height = 8,
	fig.width = 10,
	echo = TRUE,
	warning = FALSE,
	out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, the dataframe that includes the list of all files in that directory and the complete path to each file was created. Next, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2
#### Import and describe the raw data.
The code chunk below imports and cleans the data in `homicide-data.csv`. 
```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names()
```
There are `r ncol(homicide_df)` columns and `r nrow(homicide_df)` rows in this dataframe, and the important variables included are `r colnames(homicide_df)`, which are used to describe the information of homicides in 50 large U.S. cities.

#### Create a city_state variable by mutate function.
```{r}
homicide_df =
  homicide_df %>% 
  mutate(city_state = str_c(city, ', ',state))
```

#### Summarize within cities to obtain the total number of homicides.
```{r}
homicide_total = 
  homicide_df %>% 
  group_by(city_state) %>% 
  count(city_state) %>% 
  summarize(n) 
```

####  Summarize within cities to obtain the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
```{r}
homicide_unsolved = 
  homicide_df %>% 
  mutate(
    homicide_statu =  ifelse(disposition != "Closed by arrest", "unsolved", "solved")
    ) %>%
  group_by(city_state) %>% 
  summarize(
    n_unsolved = sum(homicide_statu == "unsolved"),
    n_total = n()) 
homicide_unsolved %>% 
  knitr::kable()
```
From the table, intriguingly, there are 51 city_state listed. Considering the city "Tulsa, AL" has no unsolved cases contributing to this data, it could be removed to create a more tidy database.

#### Estimate the proportion of homicides that are unsolved for the city of Baltimore, MD.
```{r}
homicide_unsolved = 
  homicide_unsolved %>%
  filter(city_state != "Tulsa, AL") 

Bal_data = 
  homicide_unsolved %>%
  filter(city_state == "Baltimore, MD")

Bal_test = 
  prop.test(
    x = Bal_data %>% pull(n_unsolved),
    n = Bal_data %>% pull(n_total),
    alternative = c("two.sided"), conf.level = 0.95
    ) %>% 
  broom::tidy() %>%
  select(estimate,conf.low,conf.high)
Bal_test
```
According to the the resulting tidy dataframe, the estimated proportion of homicides that are unsolved for the city of Baltimore, MD is 0.6455607, and the 95% confidence interval of which is (0.6275625,0.6631599).

#### Estimate the proportion of homicides that are unsolved for each of the cities in dataset, creating a tidy dataframe with estimated proportions and CIs for each city.
```{r}
All_test = 
  homicide_unsolved %>%
  mutate(
    prop_test = map2(.x = n_unsolved, .y = n_total, ~prop.test(x = .x, n = .y)),
    prop_test = map(prop_test, broom::tidy)
    ) %>% 
  unnest() %>% 
  select(city_state, estimate, conf.low, conf.high)
All_test %>% 
  knitr::kable()
```

#### Create a plot that shows the estimates and CIs for each city.
```{r}
All_test %>%
  mutate(
    city_state = fct_reorder(city_state, estimate)
    ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point(size = 1) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.5) +
  coord_flip() +
  labs(
    x = "City, State",
    y = "Estimate proportion", 
    title = "The estimated proportion of unsolved homicides in 50 cities") +
  theme(plot.title = element_text(size = 10),text = element_text(size = 6)) 
```

## Problem 3
#### Define function "t_test" in the following chunk. Set μ=0, generating 5000 datasets from the model.
```{r}
t_test = function(sample_size, mu, sigma) {
  sample_data = tibble(
    x = rnorm(n = sample_size, mean = mu, sd = sigma)
    )
   t_result = t.test(sample_data) %>% 
    broom::tidy() %>% 
    select(estimate,p.value)
}

t_test_1 = 
  rerun(5000, t_test(sample_size = 30, mu = 0, sigma = 5)) %>% 
  bind_rows
t_test_1
```

#### Repeat the above for μ={1,2,3,4,5,6} by iteration.
```{r}
t_test_2 = 
  expand_grid(
    miu = c(1:6),
    iteration = 1:5000) %>%
  mutate(
    estimate_2 = map(.x = miu, ~t_test(sample_size = 30, mu = .x, sigma = 5))
  ) %>%
  unnest(estimate_2) %>% 
  mutate(
    reject_HO = ifelse(p.value < 0.05, 1, 0)) 
```


#### Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ
on the x axis. Describe the association between effect size and power.
```{r}
t_test_2_data =
  t_test_2 %>%
    group_by(miu) %>% 
    summarise(
      count_2 = sum(reject_HO == 1))

plot_2 = 
  t_test_2_data %>%
  ggplot(aes(x = miu, y = count_2/5000)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True value of mean",
    y = "Power of the test",
    title = "Association between effect size and power") +
    theme(plot.title = element_text(size = 10),text = element_text(size = 6)) 
plot_2
```
According to the figure, the power of the test increases as the true value of mean increases.There is a positive association between effect size and power.

#### Make a plot showing the average estimate of estimated μ on the y axis and the true value of μ on the x axis. 
```{r}

```

#### Make a second plot of the average estimate of μ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?
```{r}

```

